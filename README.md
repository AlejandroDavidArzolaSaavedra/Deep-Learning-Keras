# üìÑ Pr√°ctica: Experimentaci√≥n en Redes Neuronales - Keras

<img align="left" width="250" height="180" src="https://i.imgur.com/RsmO2hP.gif?raw=true"></a>
Esta pr√°ctica tiene como objetivo explorar y comparar diferentes configuraciones de hiperpar√°metros en redes neuronales aplicadas al conjunto de datos Iris.

Se experimentar√° con variables como el n√∫mero de capas üèóÔ∏è, el n√∫mero de neuronas por capa ü§ñ, las funciones de activaci√≥n ‚ö°, el optimizador üöÄ, la funci√≥n de p√©rdida üìâ, el n√∫mero de √©pocas ‚è≥, el tama√±o del batch üì¶, entre otros.
<div id="user-content-toc">
  <ul>
    <summary><h2 style="display: inline-block">¬°Que los experimentos comiencen! üöÄüî¨üíª</h2></summary>
  </ul>
</div>

# üë• Equipo de desarrollo (Ctrl + Click para ver los perfiles)

[![GitHub](https://img.shields.io/badge/GitHub-Andrea%20Santana%20Lopez-purple?style=flat-square&logo=github)](https://github.com/AndreaSantalos)

[![GitHub](https://img.shields.io/badge/GitHub-Alejandro%20David%20Arzola%20Saavedra-blue?style=flat-square&logo=github)](https://github.com/AlejandroDavidArzolaSaavedra)


## Conjunto de Datos Iris üå∑

El conjunto de datos Iris consta de tres clases de flores:

<ul align="center">		
  <a href="https://www.kaggle.com/datasets/uciml/iris" target="_blank">
    <img style="width:50rem"  src="https://i.imgur.com/LoELZjM.png">
  </a>
</ul>

## Experimentaci√≥n con Hiperpar√°metros

Se llevar√° a cabo la experimentaci√≥n con las siguientes variables:

1. **N√∫mero de capas:** Se probar√°n diferentes configuraciones de capas ocultas.
2. **N√∫mero de neuronas por capa:** Variar√° el n√∫mero de neuronas en cada capa oculta.
3. **Funciones de activaci√≥n:** Se evaluar√°n diferentes funciones de activaci√≥n, como ReLU, sigmoid, tanh, entre otras.
4. **Optimizador:** Se comparar√°n optimizadores como SGD, Adam, RMSprop, entre otros.
5. **Funci√≥n de p√©rdida:** Se probar√°n diversas funciones de p√©rdida, como categorical crossentropy, mean squared error, etc.
6. **N√∫mero de epoch:** Variar√° el n√∫mero de epoch durante el entrenamiento.
7. **Tama√±o del batch:** Se experimentar√° con diferentes tama√±os de lote.

## Resultados y An√°lisis

A continuaci√≥n, se presenta una tabla con los resultados obtenidos para cada conjunto de datos, destacando las configuraciones de hiperpar√°metros que ofrecieron el mejor rendimiento:

### Conjunto de Datos Iris setosa

| Configuraci√≥n                | Precisi√≥n | P√©rdida |
|------------------------------|-----------|---------|
| Configuraci√≥n 1               | 0.95      | 0.12    |
| Configuraci√≥n 2               | 0.96      | 0.10    |
| **Mejor Configuraci√≥n**       | 0.96      | 0.10    |

### Conjunto de Datos Iris versicolor

| Configuraci√≥n                | Precisi√≥n | P√©rdida |
|------------------------------|-----------|---------|
| Configuraci√≥n 1               | 0.92      | 0.15    |
| Configuraci√≥n 2               | 0.94      | 0.12    |
| **Mejor Configuraci√≥n**       | 0.94      | 0.12    |

### Conjunto de Datos Iris virginica

| Configuraci√≥n                | Precisi√≥n | P√©rdida |
|------------------------------|-----------|---------|
| Configuraci√≥n 1               | 0.89      | 0.18    |
| Configuraci√≥n 2               | 0.91      | 0.15    |
| **Mejor Configuraci√≥n**       | 0.91      | 0.15    |

## Mejores Configuraciones y An√°lisis

Las configuraciones que ofrecieron el mejor rendimiento fueron aquellas con una mayor cantidad de capas ocultas y neuronas intermedias, utilizando la funci√≥n de activaci√≥n ReLU, el optimizador Adam, la funci√≥n de p√©rdida categorical crossentropy, un n√∫mero mayor de √©pocas y un tama√±o de lote moderado.

Se observ√≥ que configuraciones m√°s complejas y profunda red neuronal resultaron en un mejor rendimiento, especialmente para el conjunto de datos Iris versicolor y Iris virginica.

## C√≥mo Ejecutar la Experimentaci√≥n

Para ejecutar la experimentaci√≥n y realizar pruebas, sigue estos pasos:

1. Clona este repositorio en tu m√°quina local:
```bash
git clone <https://github.com/AlejandroDavidArzolaSaavedra/practicas_fsi/edit/deepLearningKeras>
```
2. Navega al directorio de la pr√°ctica:
```bash
cd deepLearningKeras
```
3. Ejecuta el programa principal con los archivos modificados:
```bash
python Keras_iris.ipynb
```

Observa los resultados de las pruebas y las comparaciones entre las diferentes configuraciones de hiperpar√°metros. ¬°Experimenta y ajusta para obtener los mejores resultados! 

# ü§ù Contribuciones

Si deseas contribuir a este proyecto, si√©ntete libre de hacerlo. Puedes abrir problemas (issues) o enviar solicitudes de extracci√≥n (pull requests) para mejorar el c√≥digo o agregar nuevas caracter√≠sticas. ¬°Tu colaboraci√≥n es bienvenida!
