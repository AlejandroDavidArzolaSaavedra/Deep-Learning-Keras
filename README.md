# ğŸ“„ PrÃ¡ctica: ExperimentaciÃ³n en Redes Neuronales - Keras

<img align="left" width="250" height="180" src="https://i.imgur.com/RsmO2hP.gif?raw=true"></a>
Esta prÃ¡ctica tiene como objetivo explorar y comparar diferentes configuraciones de hiperparÃ¡metros en redes neuronales aplicadas al conjunto de datos Iris.

Se experimentarÃ¡ con variables como el nÃºmero de capas ğŸ—ï¸, el nÃºmero de neuronas por capa ğŸ¤–, las funciones de activaciÃ³n âš¡, el optimizador ğŸš€, la funciÃ³n de pÃ©rdida ğŸ“‰, el nÃºmero de Ã©pocas â³, el tamaÃ±o del batch ğŸ“¦, entre otros.
<div id="user-content-toc">
  <ul>
    <summary><h2 style="display: inline-block">Â¡Que los experimentos comiencen! ğŸš€ğŸ”¬ğŸ’»</h2></summary>
  </ul>
</div>

# ğŸ‘¥ Equipo de desarrollo (Ctrl + Click para ver los perfiles)

[![GitHub](https://img.shields.io/badge/GitHub-Andrea%20Santana%20Lopez-purple?style=flat-square&logo=github)](https://github.com/AndreaSantalos)

[![GitHub](https://img.shields.io/badge/GitHub-Alejandro%20David%20Arzola%20Saavedra-blue?style=flat-square&logo=github)](https://github.com/AlejandroDavidArzolaSaavedra)


## Conjunto de Datos Iris ğŸŒ·

El conjunto de datos Iris consta de tres clases de flores:

<ul align="center">		
  <a href="https://www.kaggle.com/datasets/uciml/iris" target="_blank">
    <img style="width:50rem"  src="https://i.imgur.com/LoELZjM.png">
  </a>
</ul>

## ExperimentaciÃ³n con HiperparÃ¡metros

Se llevarÃ¡ a cabo la experimentaciÃ³n con las siguientes variables:

1. **NÃºmero de capas:** Se probarÃ¡n diferentes configuraciones de capas ocultas.
2. **NÃºmero de neuronas por capa:** VariarÃ¡ el nÃºmero de neuronas en cada capa oculta.
3. **Funciones de activaciÃ³n:** Se evaluarÃ¡n diferentes funciones de activaciÃ³n, como ReLU, sigmoid, tanh, entre otras.
4. **Optimizador:** Se compararÃ¡n optimizadores como SGD, Adam, RMSprop, entre otros.
5. **FunciÃ³n de pÃ©rdida:** Se probarÃ¡n diversas funciones de pÃ©rdida, como categorical crossentropy, mean squared error, etc.
6. **NÃºmero de epoch:** VariarÃ¡ el nÃºmero de epoch durante el entrenamiento.
7. **TamaÃ±o del batch:** Se experimentarÃ¡ con diferentes tamaÃ±os de lote.

## Mejores Configuraciones y AnÃ¡lisis

Las configuraciones que ofrecieron el mejor rendimiento fueron aquellas con una mayor cantidad de capas ocultas y neuronas intermedias, utilizando la funciÃ³n de activaciÃ³n ReLU, el optimizador Adam, la funciÃ³n de pÃ©rdida categorical crossentropy, un nÃºmero mayor de Ã©pocas y un tamaÃ±o de lote moderado.

Se observÃ³ que configuraciones mÃ¡s complejas y profunda red neuronal resultaron en un mejor rendimiento, especialmente para el conjunto de datos Iris versicolor y Iris virginica.

## CÃ³mo Ejecutar la ExperimentaciÃ³n

Para ejecutar la experimentaciÃ³n y realizar pruebas, sigue estos pasos:

1. Clona este repositorio en tu mÃ¡quina local:
```bash
git clone <https://github.com/AlejandroDavidArzolaSaavedra/practicas_fsi/edit/deepLearningKeras>
```
2. Navega al directorio de la prÃ¡ctica:
```bash
cd deepLearningKeras
```
3. Ejecuta el programa principal con los archivos modificados:
```bash
python Keras_iris.ipynb
```

Observa los resultados de las pruebas y las comparaciones entre las diferentes configuraciones de hiperparÃ¡metros. Â¡Experimenta y ajusta para obtener los mejores resultados! 

# ğŸ¤ Contribuciones

Si deseas contribuir a este proyecto, siÃ©ntete libre de hacerlo. Puedes abrir problemas (issues) o enviar solicitudes de extracciÃ³n (pull requests) para mejorar el cÃ³digo o agregar nuevas caracterÃ­sticas. Â¡Tu colaboraciÃ³n es bienvenida!
